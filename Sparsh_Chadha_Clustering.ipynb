{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster assignments saved to Customer_Clusters.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "def load_datasets():\n",
    "    customers = pd.read_csv(\"Customers.csv\")\n",
    "    products = pd.read_csv(\"Products.csv\")\n",
    "    transactions = pd.read_csv(\"Transactions.csv\")\n",
    "    return customers, products, transactions\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(customers, products, transactions):\n",
    "    # Convert date columns to datetime objects\n",
    "    customers['SignupDate'] = pd.to_datetime(customers['SignupDate'])\n",
    "    transactions['TransactionDate'] = pd.to_datetime(transactions['TransactionDate'])\n",
    "\n",
    "    # Merge datasets to create a combined dataset\n",
    "    combined_data = transactions.merge(customers, on='CustomerID').merge(products, on='ProductID')\n",
    "\n",
    "    # Calculate customer-level transaction metrics\n",
    "    customer_metrics = combined_data.groupby('CustomerID').agg(\n",
    "        AvgTransactionValue=('TotalValue', 'mean'),\n",
    "        TotalQuantity=('Quantity', 'sum'),\n",
    "        NumTransactions=('TransactionID', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate the number of days since the first signup date\n",
    "    first_signup_date = customers['SignupDate'].min()\n",
    "    customer_metrics['SignupDays'] = (customer_metrics['CustomerID'].map(customers.set_index('CustomerID')['SignupDate']) - first_signup_date).dt.days\n",
    "\n",
    "    # Calculate customer category preferences\n",
    "    category_pivot = combined_data.groupby(['CustomerID', 'Category']).size().reset_index(name='Count')\n",
    "    category_pivot = category_pivot.pivot(index='CustomerID', columns='Category', values='Count').fillna(0)\n",
    "\n",
    "    # Normalize to get the proportion of each category purchased by the customer\n",
    "    category_pivot = category_pivot.div(category_pivot.sum(axis=1), axis=0).reset_index()\n",
    "\n",
    "    # Merge all customer features\n",
    "    customer_data = customer_metrics.merge(category_pivot, on='CustomerID', how='left').fillna(0)\n",
    "    customer_data = customer_data.merge(customers[['CustomerID', 'Region']], on='CustomerID', how='left')\n",
    "\n",
    "    # Define feature groups\n",
    "    numerical_features = ['SignupDays', 'AvgTransactionValue', 'TotalQuantity', 'NumTransactions']\n",
    "    categorical_features = ['Region']\n",
    "    category_features = list(category_pivot.columns[1:])  # Exclude 'CustomerID'\n",
    "    all_features = numerical_features + categorical_features + category_features\n",
    "\n",
    "    # Preprocessor for scaling and encoding\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep category features\n",
    "    )\n",
    "\n",
    "    # Apply preprocessing\n",
    "    processed_features = preprocessor.fit_transform(customer_data[all_features])\n",
    "\n",
    "    # Create DataFrame with processed features\n",
    "    processed_features_df = pd.DataFrame(\n",
    "        processed_features,\n",
    "        columns=preprocessor.get_feature_names_out(),\n",
    "        index=customer_data['CustomerID']\n",
    "    )\n",
    "\n",
    "    return processed_features_df, customer_data\n",
    "\n",
    "# Function to determine optimal number of clusters using Elbow method\n",
    "def find_optimal_clusters(data, max_clusters=30):\n",
    "    wcss = []\n",
    "    for i in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)\n",
    "        kmeans.fit(data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    plt.plot(range(2, max_clusters + 1), wcss, marker='o')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate clusters\n",
    "def evaluate_clusters(data, labels):\n",
    "    db_index = davies_bouldin_score(data, labels)\n",
    "    silhouette = silhouette_score(data, labels)\n",
    "    print(f'Davies-Bouldin Index: {db_index:.3f}')\n",
    "    print(f'Silhouette Score: {silhouette:.3f}')\n",
    "    return db_index, silhouette\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_customers_by_cluster(data, labels, n_clusters, algorithm):\n",
    "    \"\"\"\n",
    "    Visualize customers by cluster.\n",
    "    \n",
    "    Args:\n",
    "        data (array-like): Preprocessed feature data.\n",
    "        labels (array-like): Cluster labels for each customer.\n",
    "        n_clusters (int): Number of clusters.\n",
    "        algorithm (str): Name of the clustering algorithm.\n",
    "    \"\"\"\n",
    "    # Add labels to the original customer data\n",
    "    customer_data['Cluster'] = labels\n",
    "\n",
    "    # Reduce dimensions using PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "\n",
    "    # Create a DataFrame with reduced dimensions and cluster labels\n",
    "    plot_data = pd.DataFrame(reduced_data, columns=['PCA1', 'PCA2'])\n",
    "    plot_data['Cluster'] = labels\n",
    "    plot_data['CustomerID'] = customer_data['CustomerID'].values\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(data=plot_data, x='PCA1', y='PCA2', hue='Cluster', palette='viridis', s=100)\n",
    "    plt.title(f'Customer Clusters ({algorithm})', fontsize=16)\n",
    "    plt.xlabel('Principal Component 1', fontsize=12)\n",
    "    plt.ylabel('Principal Component 2', fontsize=12)\n",
    "\n",
    "    # Add cluster centers if K-Means\n",
    "    if algorithm == 'kmeans':\n",
    "        kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42, n_init=10)\n",
    "        kmeans.fit(data)\n",
    "        centers = pca.transform(kmeans.cluster_centers_)\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', label='Cluster Centers')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to perform clustering using a selected algorithm\n",
    "def perform_clustering(data, algorithm, **kwargs):\n",
    "    if algorithm == 'kmeans':\n",
    "        model = KMeans(**kwargs)\n",
    "    elif algorithm == 'hierarchical':\n",
    "        model = AgglomerativeClustering(**kwargs)\n",
    "    elif algorithm == 'dbscan':\n",
    "        model = DBSCAN(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported clustering algorithm. Choose from 'kmeans', 'hierarchical', or 'dbscan'.\")\n",
    "\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "def save_cluster_assignments(customer_data, labels, output_file=\"Customer_Clusters.csv\"):\n",
    "    \"\"\"\n",
    "    Save customer IDs along with their cluster assignments to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        customer_data (pd.DataFrame): DataFrame containing customer information.\n",
    "        labels (array-like): Cluster labels for each customer.\n",
    "        output_file (str): Name of the output CSV file.\n",
    "    \"\"\"\n",
    "    # Add cluster labels to the customer data\n",
    "    customer_data['Cluster'] = labels\n",
    "\n",
    "    # Select only the CustomerID and Cluster columns for output\n",
    "    cluster_assignments = customer_data[['CustomerID', 'Cluster']]\n",
    "\n",
    "    # Save to CSV\n",
    "    cluster_assignments.to_csv(output_file, index=False)\n",
    "    print(f\"Cluster assignments saved to {output_file}.\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    customers, products, transactions = load_datasets()\n",
    "    processed_features, customer_data = preprocess_data(customers, products, transactions)\n",
    "\n",
    "    # Perform K-Means clustering\n",
    "    kmeans_labels = perform_clustering(processed_features, 'kmeans', n_clusters=5, init='k-means++', random_state=42, n_init=10)\n",
    "    \n",
    "    # Save cluster assignments to a CSV file\n",
    "    save_cluster_assignments(customer_data, kmeans_labels, output_file=\"Customer_Clusters.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
